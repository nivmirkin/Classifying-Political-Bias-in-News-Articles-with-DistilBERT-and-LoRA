{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ch3bxb-K0dZe"
      },
      "outputs": [],
      "source": [
        "# Install necessary libraries and clone the dataset\n",
        "!git clone https://github.com/ramybaly/Article-Bias-Prediction.git  # Cloning the dataset repository\n",
        "\n",
        "# Install required libraries\n",
        "!pip install optuna transformers peft datasets scikit-learn evaluate\n",
        "\n",
        "# Import necessary libraries\n",
        "import os\n",
        "import json\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import optuna\n",
        "from datasets import Dataset\n",
        "from sklearn.model_selection import train_test_split\n",
        "from transformers import AutoTokenizer, BertForSequenceClassification, TrainingArguments, Trainer\n",
        "from peft import get_peft_model, LoraConfig, TaskType\n",
        "from sklearn.metrics import confusion_matrix, classification_report, ConfusionMatrixDisplay\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Change directory to the cloned dataset\n",
        "%cd Article-Bias-Prediction\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def print_number_of_trainable_model_parameters(model):\n",
        "    \"\"\"\n",
        "    Print the number of trainable and total parameters in the model, and the percentage of trainable parameters.\n",
        "\n",
        "    Args:\n",
        "        model: A PyTorch model whose parameters are to be analyzed.\n",
        "\n",
        "    Returns:\n",
        "        str: A formatted string with the counts and percentage of trainable parameters.\n",
        "    \"\"\"\n",
        "    trainable_model_params = 0\n",
        "    all_model_params = 0\n",
        "\n",
        "    for _, param in model.named_parameters():\n",
        "        all_model_params += param.numel()  # Total number of parameters\n",
        "        if param.requires_grad:  # Check if the parameter is trainable\n",
        "            trainable_model_params += param.numel()\n",
        "\n",
        "    percentage_trainable = 100 * trainable_model_params / all_model_params if all_model_params > 0 else 0\n",
        "\n",
        "    return (f\"Trainable model parameters: {trainable_model_params}\\n\"\n",
        "            f\"All model parameters: {all_model_params}\\n\"\n",
        "            f\"Percentage of trainable model parameters: {percentage_trainable:.2f}%\")\n"
      ],
      "metadata": {
        "id": "8HoM2PAi0iSv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Load and prepare data\n",
        "json_dir = 'data/jsons'\n",
        "data_list = []\n",
        "\n",
        "\n",
        "for filename in os.listdir(json_dir):\n",
        "    if filename.endswith('.json'):\n",
        "        with open(os.path.join(json_dir, filename), 'r') as file:\n",
        "            data = json.load(file)\n",
        "            # Change 'bias' to 'label' in each JSON object\n",
        "            if 'bias' in data:\n",
        "                data['labels'] = data.pop('bias')\n",
        "            data_list.append(data)\n",
        "# Convert to DataFrame\n",
        "df = pd.DataFrame(data_list)\n",
        "\n",
        "# Define column names\n",
        "text_column = 'content_original'\n",
        "label_column = 'bias_text'\n",
        "\n",
        "# Map bias_text to numerical labels\n",
        "label_map = {'left': 0, 'center': 1, 'right': 2}\n",
        "df[label_column] = df[label_column].map(label_map)\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "train_texts, test_texts, train_labels, test_labels = train_test_split(\n",
        "    df['content_original'], df['labels'], test_size=0.15, stratify=df['labels'], random_state=42\n",
        ")\n",
        "\n",
        "# Combine the training texts and labels into a single DataFrame\n",
        "train_df = pd.DataFrame({'content_original': train_texts, 'labels': train_labels})\n",
        "test_df = pd.DataFrame({text_column: test_texts,  'labels': test_labels})\n",
        "\n",
        "train_df.reset_index(drop=True, inplace=True)\n",
        "test_df.reset_index(drop=True, inplace=True)\n",
        "# Split the training set into training and validation sets\n",
        "train_df, val_df = train_test_split(train_df, test_size=3/17, random_state=42)\n",
        "val_df.reset_index(drop=True, inplace=True)\n",
        "train_df.reset_index(drop=True, inplace=True)\n",
        "# Display the shapes of the resulting DataFrames\n",
        "print(f\"Training set shape: {train_df.shape}\")\n",
        "print(f\"Validation set shape: {val_df.shape}\")\n",
        "print(f\"Test set shape:{test_df.shape}\")\n",
        "\n",
        "print(train_df.head())\n"
      ],
      "metadata": {
        "id": "oEIVElEz0uVY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import DistilBertForSequenceClassification, AutoTokenizer\n",
        "\n",
        "# Load the DistilBERT model for sequence classification\n",
        "model = DistilBertForSequenceClassification.from_pretrained('distilbert-base-cased', num_labels=3)\n",
        "\n",
        "# Load the tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-cased\")\n",
        "\n",
        "# Print the number of trainable parameters in the original model\n",
        "print(print_number_of_trainable_model_parameters(model))\n",
        "\n",
        "from peft import LoraConfig, TaskType, get_peft_model\n",
        "\n",
        "# Define the LoRA configuration\n",
        "lora_config = LoraConfig(\n",
        "    r=64,                # Rank Number\n",
        "    lora_alpha=64,       # Alpha (Scaling Factor)\n",
        "    lora_dropout=0.0,    # Dropout Probability for LoRA\n",
        "    target_modules=[\"q_lin\", \"k_lin\", \"v_lin\"],  # Target modules in the model to apply LoRA (usually MultiHead Attention Layers)\n",
        "    bias='none',         # No bias in the LoRA layers\n",
        "    task_type=TaskType.SEQ_CLS  # Sequence to Classification Task\n",
        ")\n",
        "\n",
        "# Apply the LoRA configuration to the model\n",
        "peft_model = get_peft_model(model, lora_config)\n",
        "\n",
        "# Print the number of trainable parameters in the LoRA-adapted model\n",
        "print(print_number_of_trainable_model_parameters(peft_model))"
      ],
      "metadata": {
        "id": "BYKGjXnw6xai"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenize function\n",
        "def tokenize_func(data):\n",
        "    return tokenizer(\n",
        "        data[text_column],\n",
        "        max_length=512,\n",
        "        padding='max_length',\n",
        "        return_attention_mask=True,\n",
        "        truncation=True\n",
        "    )\n",
        "from datasets import Dataset\n",
        "# Convert DataFrame to Dataset\n",
        "# Convert DataFrames to Datasets\n",
        "train_dataset = Dataset.from_pandas(train_df)\n",
        "val_dataset = Dataset.from_pandas(val_df)\n",
        "test_dataset = Dataset.from_pandas(test_df)\n",
        "\n",
        "# Tokenize the datasets\n",
        "train_dataset = train_dataset.map(\n",
        "    tokenize_func,\n",
        "    batched=True,\n",
        "    remove_columns=[text_column]\n",
        ")\n",
        "val_dataset = val_dataset.map(\n",
        "    tokenize_func,\n",
        "    batched=True,\n",
        "    remove_columns=[text_column]\n",
        ")\n",
        "test_dataset = test_dataset.map(\n",
        "    tokenize_func,\n",
        "    batched=True,\n",
        "    remove_columns=[text_column]\n",
        ")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "PhafvSnY631O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AdamW, get_linear_schedule_with_warmup, DataCollatorWithPadding, Adafactor\n",
        "import math\n",
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
        "\n",
        "# Define evaluation metrics\n",
        "def metrics(eval_prediction):\n",
        "    logits, labels = eval_prediction\n",
        "    pred = np.argmax(logits, axis=1)\n",
        "    precision, recall, f1, _ = precision_recall_fscore_support(labels, pred, average='macro')\n",
        "    accuracy = accuracy_score(labels, pred)\n",
        "    return {\"accuracy\": accuracy, \"f1\": f1}\n",
        "\n",
        "# Training parameters\n",
        "train_batch_size = 32\n",
        "eval_batch_size = 32\n",
        "lr = 5e-4\n",
        "\n",
        "# Define training arguments\n",
        "peft_training_args = TrainingArguments(\n",
        "    output_dir='./result-distilbert-lora',\n",
        "    logging_dir='./logs-distilbert-lora',\n",
        "    learning_rate=lr,\n",
        "    per_device_train_batch_size=train_batch_size,  # Adjust based on GPU memory\n",
        "    per_device_eval_batch_size=eval_batch_size,    # Adjust based on GPU memory\n",
        "    num_train_epochs=5,\n",
        "    logging_steps=500,\n",
        "    evaluation_strategy='steps',\n",
        "    eval_steps=500,\n",
        "    weight_decay=0.01,\n",
        "    seed=42,\n",
        "    fp16=True,  # Only use with GPU\n",
        "    report_to='none'\n",
        ")\n",
        "\n",
        "# Define optimizer\n",
        "optimizer = AdamW(peft_model.parameters(), lr=lr)\n",
        "\n",
        "# Optionally replace AdamW with Adafactor\n",
        "# optimizer = Adafactor(\n",
        "#     peft_model.parameters(),\n",
        "#     lr=lr,\n",
        "#     eps=(1e-30, 1e-3),\n",
        "#     clip_threshold=1.0,\n",
        "#     decay_rate=-0.8,\n",
        "#     beta1=None,\n",
        "#     weight_decay=0.0,\n",
        "#     relative_step=False,\n",
        "#     scale_parameter=False,\n",
        "#     warmup_init=False,\n",
        "# )\n",
        "\n",
        "# Define scheduler\n",
        "n_epochs = peft_training_args.num_train_epochs\n",
        "total_steps = n_epochs * math.ceil(len(train_dataset) / train_batch_size)\n",
        "lr_scheduler = get_linear_schedule_with_warmup(\n",
        "    optimizer,\n",
        "    num_warmup_steps=0,\n",
        "    num_training_steps=total_steps\n",
        ")\n",
        "\n",
        "# Data collator\n",
        "collator = DataCollatorWithPadding(\n",
        "    tokenizer=tokenizer,\n",
        "    padding=\"longest\"\n",
        ")\n",
        "\n",
        "# Define Trainer\n",
        "peft_trainer = Trainer(\n",
        "    model=peft_model,\n",
        "    args=peft_training_args,\n",
        "    train_dataset=train_dataset,  # Training Data\n",
        "    eval_dataset=val_dataset,     # Evaluation Data\n",
        "    tokenizer=tokenizer,\n",
        "    compute_metrics=metrics,\n",
        "    optimizers=(optimizer, lr_scheduler),\n",
        "    data_collator=collator\n",
        ")\n",
        "\n",
        "print(f\"Total Steps: {total_steps}\")\n",
        "\n",
        "# Path to save the fine-tuned model\n",
        "peft_model_path = \"./\"\n",
        "\n",
        "# Train the model\n",
        "peft_trainer.train()\n",
        "\n",
        "# Save the fine-tuned model and tokenizer\n",
        "peft_trainer.model.save_pretrained(peft_model_path)\n",
        "tokenizer.save_pretrained(peft_model_path)\n"
      ],
      "metadata": {
        "id": "0vIKacfm1sLT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the model on the test dataset\n",
        "test_results = peft_trainer.evaluate(test_dataset)\n",
        "print(f\"Test Results: {test_results}\")\n",
        "\n",
        "# Use the predict method to get predictions and labels\n",
        "predictions, labels, _ = peft_trainer.predict(test_dataset)\n",
        "\n",
        "# Convert predictions to class labels\n",
        "preds = np.argmax(predictions, axis=1)\n",
        "\n",
        "# Calculate the confusion matrix\n",
        "cm = confusion_matrix(labels, preds)\n",
        "\n",
        "# Display the confusion matrix\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
        "disp.plot()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "qYujyipj14ml"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}